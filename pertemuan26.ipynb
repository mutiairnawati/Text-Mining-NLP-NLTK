{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print( os.listdir( nltk.data.find(\"corpora\") ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01',\n",
       " 'ca02',\n",
       " 'ca03',\n",
       " 'ca04',\n",
       " 'ca05',\n",
       " 'ca06',\n",
       " 'ca07',\n",
       " 'ca08',\n",
       " 'ca09',\n",
       " 'ca10',\n",
       " 'ca11',\n",
       " 'ca12',\n",
       " 'ca13',\n",
       " 'ca14',\n",
       " 'ca15',\n",
       " 'ca16',\n",
       " 'ca17',\n",
       " 'ca18',\n",
       " 'ca19',\n",
       " 'ca20',\n",
       " 'ca21',\n",
       " 'ca22',\n",
       " 'ca23',\n",
       " 'ca24',\n",
       " 'ca25',\n",
       " 'ca26',\n",
       " 'ca27',\n",
       " 'ca28',\n",
       " 'ca29',\n",
       " 'ca30',\n",
       " 'ca31',\n",
       " 'ca32',\n",
       " 'ca33',\n",
       " 'ca34',\n",
       " 'ca35',\n",
       " 'ca36',\n",
       " 'ca37',\n",
       " 'ca38',\n",
       " 'ca39',\n",
       " 'ca40',\n",
       " 'ca41',\n",
       " 'ca42',\n",
       " 'ca43',\n",
       " 'ca44',\n",
       " 'cb01',\n",
       " 'cb02',\n",
       " 'cb03',\n",
       " 'cb04',\n",
       " 'cb05',\n",
       " 'cb06',\n",
       " 'cb07',\n",
       " 'cb08',\n",
       " 'cb09',\n",
       " 'cb10',\n",
       " 'cb11',\n",
       " 'cb12',\n",
       " 'cb13',\n",
       " 'cb14',\n",
       " 'cb15',\n",
       " 'cb16',\n",
       " 'cb17',\n",
       " 'cb18',\n",
       " 'cb19',\n",
       " 'cb20',\n",
       " 'cb21',\n",
       " 'cb22',\n",
       " 'cb23',\n",
       " 'cb24',\n",
       " 'cb25',\n",
       " 'cb26',\n",
       " 'cb27',\n",
       " 'cc01',\n",
       " 'cc02',\n",
       " 'cc03',\n",
       " 'cc04',\n",
       " 'cc05',\n",
       " 'cc06',\n",
       " 'cc07',\n",
       " 'cc08',\n",
       " 'cc09',\n",
       " 'cc10',\n",
       " 'cc11',\n",
       " 'cc12',\n",
       " 'cc13',\n",
       " 'cc14',\n",
       " 'cc15',\n",
       " 'cc16',\n",
       " 'cc17',\n",
       " 'cd01',\n",
       " 'cd02',\n",
       " 'cd03',\n",
       " 'cd04',\n",
       " 'cd05',\n",
       " 'cd06',\n",
       " 'cd07',\n",
       " 'cd08',\n",
       " 'cd09',\n",
       " 'cd10',\n",
       " 'cd11',\n",
       " 'cd12',\n",
       " 'cd13',\n",
       " 'cd14',\n",
       " 'cd15',\n",
       " 'cd16',\n",
       " 'cd17',\n",
       " 'ce01',\n",
       " 'ce02',\n",
       " 'ce03',\n",
       " 'ce04',\n",
       " 'ce05',\n",
       " 'ce06',\n",
       " 'ce07',\n",
       " 'ce08',\n",
       " 'ce09',\n",
       " 'ce10',\n",
       " 'ce11',\n",
       " 'ce12',\n",
       " 'ce13',\n",
       " 'ce14',\n",
       " 'ce15',\n",
       " 'ce16',\n",
       " 'ce17',\n",
       " 'ce18',\n",
       " 'ce19',\n",
       " 'ce20',\n",
       " 'ce21',\n",
       " 'ce22',\n",
       " 'ce23',\n",
       " 'ce24',\n",
       " 'ce25',\n",
       " 'ce26',\n",
       " 'ce27',\n",
       " 'ce28',\n",
       " 'ce29',\n",
       " 'ce30',\n",
       " 'ce31',\n",
       " 'ce32',\n",
       " 'ce33',\n",
       " 'ce34',\n",
       " 'ce35',\n",
       " 'ce36',\n",
       " 'cf01',\n",
       " 'cf02',\n",
       " 'cf03',\n",
       " 'cf04',\n",
       " 'cf05',\n",
       " 'cf06',\n",
       " 'cf07',\n",
       " 'cf08',\n",
       " 'cf09',\n",
       " 'cf10',\n",
       " 'cf11',\n",
       " 'cf12',\n",
       " 'cf13',\n",
       " 'cf14',\n",
       " 'cf15',\n",
       " 'cf16',\n",
       " 'cf17',\n",
       " 'cf18',\n",
       " 'cf19',\n",
       " 'cf20',\n",
       " 'cf21',\n",
       " 'cf22',\n",
       " 'cf23',\n",
       " 'cf24',\n",
       " 'cf25',\n",
       " 'cf26',\n",
       " 'cf27',\n",
       " 'cf28',\n",
       " 'cf29',\n",
       " 'cf30',\n",
       " 'cf31',\n",
       " 'cf32',\n",
       " 'cf33',\n",
       " 'cf34',\n",
       " 'cf35',\n",
       " 'cf36',\n",
       " 'cf37',\n",
       " 'cf38',\n",
       " 'cf39',\n",
       " 'cf40',\n",
       " 'cf41',\n",
       " 'cf42',\n",
       " 'cf43',\n",
       " 'cf44',\n",
       " 'cf45',\n",
       " 'cf46',\n",
       " 'cf47',\n",
       " 'cf48',\n",
       " 'cg01',\n",
       " 'cg02',\n",
       " 'cg03',\n",
       " 'cg04',\n",
       " 'cg05',\n",
       " 'cg06',\n",
       " 'cg07',\n",
       " 'cg08',\n",
       " 'cg09',\n",
       " 'cg10',\n",
       " 'cg11',\n",
       " 'cg12',\n",
       " 'cg13',\n",
       " 'cg14',\n",
       " 'cg15',\n",
       " 'cg16',\n",
       " 'cg17',\n",
       " 'cg18',\n",
       " 'cg19',\n",
       " 'cg20',\n",
       " 'cg21',\n",
       " 'cg22',\n",
       " 'cg23',\n",
       " 'cg24',\n",
       " 'cg25',\n",
       " 'cg26',\n",
       " 'cg27',\n",
       " 'cg28',\n",
       " 'cg29',\n",
       " 'cg30',\n",
       " 'cg31',\n",
       " 'cg32',\n",
       " 'cg33',\n",
       " 'cg34',\n",
       " 'cg35',\n",
       " 'cg36',\n",
       " 'cg37',\n",
       " 'cg38',\n",
       " 'cg39',\n",
       " 'cg40',\n",
       " 'cg41',\n",
       " 'cg42',\n",
       " 'cg43',\n",
       " 'cg44',\n",
       " 'cg45',\n",
       " 'cg46',\n",
       " 'cg47',\n",
       " 'cg48',\n",
       " 'cg49',\n",
       " 'cg50',\n",
       " 'cg51',\n",
       " 'cg52',\n",
       " 'cg53',\n",
       " 'cg54',\n",
       " 'cg55',\n",
       " 'cg56',\n",
       " 'cg57',\n",
       " 'cg58',\n",
       " 'cg59',\n",
       " 'cg60',\n",
       " 'cg61',\n",
       " 'cg62',\n",
       " 'cg63',\n",
       " 'cg64',\n",
       " 'cg65',\n",
       " 'cg66',\n",
       " 'cg67',\n",
       " 'cg68',\n",
       " 'cg69',\n",
       " 'cg70',\n",
       " 'cg71',\n",
       " 'cg72',\n",
       " 'cg73',\n",
       " 'cg74',\n",
       " 'cg75',\n",
       " 'ch01',\n",
       " 'ch02',\n",
       " 'ch03',\n",
       " 'ch04',\n",
       " 'ch05',\n",
       " 'ch06',\n",
       " 'ch07',\n",
       " 'ch08',\n",
       " 'ch09',\n",
       " 'ch10',\n",
       " 'ch11',\n",
       " 'ch12',\n",
       " 'ch13',\n",
       " 'ch14',\n",
       " 'ch15',\n",
       " 'ch16',\n",
       " 'ch17',\n",
       " 'ch18',\n",
       " 'ch19',\n",
       " 'ch20',\n",
       " 'ch21',\n",
       " 'ch22',\n",
       " 'ch23',\n",
       " 'ch24',\n",
       " 'ch25',\n",
       " 'ch26',\n",
       " 'ch27',\n",
       " 'ch28',\n",
       " 'ch29',\n",
       " 'ch30',\n",
       " 'cj01',\n",
       " 'cj02',\n",
       " 'cj03',\n",
       " 'cj04',\n",
       " 'cj05',\n",
       " 'cj06',\n",
       " 'cj07',\n",
       " 'cj08',\n",
       " 'cj09',\n",
       " 'cj10',\n",
       " 'cj11',\n",
       " 'cj12',\n",
       " 'cj13',\n",
       " 'cj14',\n",
       " 'cj15',\n",
       " 'cj16',\n",
       " 'cj17',\n",
       " 'cj18',\n",
       " 'cj19',\n",
       " 'cj20',\n",
       " 'cj21',\n",
       " 'cj22',\n",
       " 'cj23',\n",
       " 'cj24',\n",
       " 'cj25',\n",
       " 'cj26',\n",
       " 'cj27',\n",
       " 'cj28',\n",
       " 'cj29',\n",
       " 'cj30',\n",
       " 'cj31',\n",
       " 'cj32',\n",
       " 'cj33',\n",
       " 'cj34',\n",
       " 'cj35',\n",
       " 'cj36',\n",
       " 'cj37',\n",
       " 'cj38',\n",
       " 'cj39',\n",
       " 'cj40',\n",
       " 'cj41',\n",
       " 'cj42',\n",
       " 'cj43',\n",
       " 'cj44',\n",
       " 'cj45',\n",
       " 'cj46',\n",
       " 'cj47',\n",
       " 'cj48',\n",
       " 'cj49',\n",
       " 'cj50',\n",
       " 'cj51',\n",
       " 'cj52',\n",
       " 'cj53',\n",
       " 'cj54',\n",
       " 'cj55',\n",
       " 'cj56',\n",
       " 'cj57',\n",
       " 'cj58',\n",
       " 'cj59',\n",
       " 'cj60',\n",
       " 'cj61',\n",
       " 'cj62',\n",
       " 'cj63',\n",
       " 'cj64',\n",
       " 'cj65',\n",
       " 'cj66',\n",
       " 'cj67',\n",
       " 'cj68',\n",
       " 'cj69',\n",
       " 'cj70',\n",
       " 'cj71',\n",
       " 'cj72',\n",
       " 'cj73',\n",
       " 'cj74',\n",
       " 'cj75',\n",
       " 'cj76',\n",
       " 'cj77',\n",
       " 'cj78',\n",
       " 'cj79',\n",
       " 'cj80',\n",
       " 'ck01',\n",
       " 'ck02',\n",
       " 'ck03',\n",
       " 'ck04',\n",
       " 'ck05',\n",
       " 'ck06',\n",
       " 'ck07',\n",
       " 'ck08',\n",
       " 'ck09',\n",
       " 'ck10',\n",
       " 'ck11',\n",
       " 'ck12',\n",
       " 'ck13',\n",
       " 'ck14',\n",
       " 'ck15',\n",
       " 'ck16',\n",
       " 'ck17',\n",
       " 'ck18',\n",
       " 'ck19',\n",
       " 'ck20',\n",
       " 'ck21',\n",
       " 'ck22',\n",
       " 'ck23',\n",
       " 'ck24',\n",
       " 'ck25',\n",
       " 'ck26',\n",
       " 'ck27',\n",
       " 'ck28',\n",
       " 'ck29',\n",
       " 'cl01',\n",
       " 'cl02',\n",
       " 'cl03',\n",
       " 'cl04',\n",
       " 'cl05',\n",
       " 'cl06',\n",
       " 'cl07',\n",
       " 'cl08',\n",
       " 'cl09',\n",
       " 'cl10',\n",
       " 'cl11',\n",
       " 'cl12',\n",
       " 'cl13',\n",
       " 'cl14',\n",
       " 'cl15',\n",
       " 'cl16',\n",
       " 'cl17',\n",
       " 'cl18',\n",
       " 'cl19',\n",
       " 'cl20',\n",
       " 'cl21',\n",
       " 'cl22',\n",
       " 'cl23',\n",
       " 'cl24',\n",
       " 'cm01',\n",
       " 'cm02',\n",
       " 'cm03',\n",
       " 'cm04',\n",
       " 'cm05',\n",
       " 'cm06',\n",
       " 'cn01',\n",
       " 'cn02',\n",
       " 'cn03',\n",
       " 'cn04',\n",
       " 'cn05',\n",
       " 'cn06',\n",
       " 'cn07',\n",
       " 'cn08',\n",
       " 'cn09',\n",
       " 'cn10',\n",
       " 'cn11',\n",
       " 'cn12',\n",
       " 'cn13',\n",
       " 'cn14',\n",
       " 'cn15',\n",
       " 'cn16',\n",
       " 'cn17',\n",
       " 'cn18',\n",
       " 'cn19',\n",
       " 'cn20',\n",
       " 'cn21',\n",
       " 'cn22',\n",
       " 'cn23',\n",
       " 'cn24',\n",
       " 'cn25',\n",
       " 'cn26',\n",
       " 'cn27',\n",
       " 'cn28',\n",
       " 'cn29',\n",
       " 'cp01',\n",
       " 'cp02',\n",
       " 'cp03',\n",
       " 'cp04',\n",
       " 'cp05',\n",
       " 'cp06',\n",
       " 'cp07',\n",
       " 'cp08',\n",
       " 'cp09',\n",
       " 'cp10',\n",
       " 'cp11',\n",
       " 'cp12',\n",
       " 'cp13',\n",
       " 'cp14',\n",
       " 'cp15',\n",
       " 'cp16',\n",
       " 'cp17',\n",
       " 'cp18',\n",
       " 'cp19',\n",
       " 'cp20',\n",
       " 'cp21',\n",
       " 'cp22',\n",
       " 'cp23',\n",
       " 'cp24',\n",
       " 'cp25',\n",
       " 'cp26',\n",
       " 'cp27',\n",
       " 'cp28',\n",
       " 'cp29',\n",
       " 'cr01',\n",
       " 'cr02',\n",
       " 'cr03',\n",
       " 'cr04',\n",
       " 'cr05',\n",
       " 'cr06',\n",
       " 'cr07',\n",
       " 'cr08',\n",
       " 'cr09']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n"
     ]
    }
   ],
   "source": [
    "hamlet=nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "print(hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:\n",
    "    print(word, sep = ' ', end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI=\"\"\"According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”. Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think. AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AI_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9ab67f77ffe1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAI_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'AI_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "type(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According', 'to', 'the', 'father', 'of', 'Artificial', 'Intelligence', ',', 'John', 'McCarthy', ',', 'it', 'is', '“', 'The', 'science', 'and', 'engineering', 'of', 'making', 'intelligent', 'machines', ',', 'especially', 'intelligent', 'computer', 'programs', '”', '.', 'Artificial', 'Intelligence', 'is', 'a', 'way', 'of', 'making', 'a', 'computer', ',', 'a', 'computer-controlled', 'robot', ',', 'or', 'a', 'software', 'think', 'intelligently', ',', 'in', 'the', 'similar', 'manner', 'the', 'intelligent', 'humans', 'think', '.', 'AI', 'is', 'accomplished', 'by', 'studying', 'how', 'human', 'brain', 'thinks', ',', 'and', 'how', 'humans', 'learn', ',', 'decide', ',', 'and', 'work', 'while', 'trying', 'to', 'solve', 'a', 'problem', ',', 'and', 'then', 'using', 'the', 'outcomes', 'of', 'this', 'study', 'as', 'a', 'basis', 'of', 'developing', 'intelligent', 'software', 'and', 'systems', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "AI_tokens = word_tokenize(AI)\n",
    "print(AI_tokens)\n",
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.', 'Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.', 'AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "AI_tokens = sent_tokenize(AI)\n",
    "print(AI_tokens)\n",
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 3 samples and 3 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist()\n",
    "for word in AI_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('according to the father of artificial intelligence, john mccarthy, it is “the science and engineering of making intelligent machines, especially intelligent computer programs”.',\n",
       "  1),\n",
       " ('artificial intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.',\n",
       "  1),\n",
       " ('ai is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.',\n",
       "  1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 29 samples and 575 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist2 = FreqDist()\n",
    "for word in AI:\n",
    "    fdist2[word.lower()]+=1\n",
    "print(fdist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 86),\n",
       " ('i', 49),\n",
       " ('e', 49),\n",
       " ('n', 45),\n",
       " ('t', 41),\n",
       " ('a', 37),\n",
       " ('o', 29),\n",
       " ('l', 28),\n",
       " ('s', 26),\n",
       " ('r', 23)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist2_top10 = fdist2.most_common(10)\n",
    "fdist2_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank=blankline_tokenize(AI)\n",
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"The best and most beautiful things in the world cannot be seen or even touched, they must be felt with heart\"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'best'), ('best', 'and'), ('and', 'most'), ('most', 'beautiful'), ('beautiful', 'things'), ('things', 'in'), ('in', 'the'), ('the', 'world'), ('world', 'can'), ('can', 'not'), ('not', 'be'), ('be', 'seen'), ('seen', 'or'), ('or', 'even'), ('even', 'touched'), ('touched', ','), (',', 'they'), ('they', 'must'), ('must', 'be'), ('be', 'felt'), ('felt', 'with'), ('with', 'heart')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quotes_bigrams)\n",
    "len(quotes_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'best', 'and'), ('best', 'and', 'most'), ('and', 'most', 'beautiful'), ('most', 'beautiful', 'things'), ('beautiful', 'things', 'in'), ('things', 'in', 'the'), ('in', 'the', 'world'), ('the', 'world', 'can'), ('world', 'can', 'not'), ('can', 'not', 'be'), ('not', 'be', 'seen'), ('be', 'seen', 'or'), ('seen', 'or', 'even'), ('or', 'even', 'touched'), ('even', 'touched', ','), ('touched', ',', 'they'), (',', 'they', 'must'), ('they', 'must', 'be'), ('must', 'be', 'felt'), ('be', 'felt', 'with'), ('felt', 'with', 'heart')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quotes_trigrams)\n",
    "len(quotes_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'best', 'and', 'most', 'beautiful'), ('best', 'and', 'most', 'beautiful', 'things'), ('and', 'most', 'beautiful', 'things', 'in'), ('most', 'beautiful', 'things', 'in', 'the'), ('beautiful', 'things', 'in', 'the', 'world'), ('things', 'in', 'the', 'world', 'can'), ('in', 'the', 'world', 'can', 'not'), ('the', 'world', 'can', 'not', 'be'), ('world', 'can', 'not', 'be', 'seen'), ('can', 'not', 'be', 'seen', 'or'), ('not', 'be', 'seen', 'or', 'even'), ('be', 'seen', 'or', 'even', 'touched'), ('seen', 'or', 'even', 'touched', ','), ('or', 'even', 'touched', ',', 'they'), ('even', 'touched', ',', 'they', 'must'), ('touched', ',', 'they', 'must', 'be'), (',', 'they', 'must', 'be', 'felt'), ('they', 'must', 'be', 'felt', 'with'), ('must', 'be', 'felt', 'with', 'heart')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quotes_ngrams)\n",
    "len(quotes_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_tokens = nltk.word_tokenize(AI)\n",
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('According', 'to'), ('to', 'the'), ('the', 'father'), ('father', 'of'), ('of', 'Artificial'), ('Artificial', 'Intelligence'), ('Intelligence', ','), (',', 'John'), ('John', 'McCarthy'), ('McCarthy', ','), (',', 'it'), ('it', 'is'), ('is', '“'), ('“', 'The'), ('The', 'science'), ('science', 'and'), ('and', 'engineering'), ('engineering', 'of'), ('of', 'making'), ('making', 'intelligent'), ('intelligent', 'machines'), ('machines', ','), (',', 'especially'), ('especially', 'intelligent'), ('intelligent', 'computer'), ('computer', 'programs'), ('programs', '”'), ('”', '.'), ('.', 'Artificial'), ('Artificial', 'Intelligence'), ('Intelligence', 'is'), ('is', 'a'), ('a', 'way'), ('way', 'of'), ('of', 'making'), ('making', 'a'), ('a', 'computer'), ('computer', ','), (',', 'a'), ('a', 'computer-controlled'), ('computer-controlled', 'robot'), ('robot', ','), (',', 'or'), ('or', 'a'), ('a', 'software'), ('software', 'think'), ('think', 'intelligently'), ('intelligently', ','), (',', 'in'), ('in', 'the'), ('the', 'similar'), ('similar', 'manner'), ('manner', 'the'), ('the', 'intelligent'), ('intelligent', 'humans'), ('humans', 'think'), ('think', '.'), ('.', 'AI'), ('AI', 'is'), ('is', 'accomplished'), ('accomplished', 'by'), ('by', 'studying'), ('studying', 'how'), ('how', 'human'), ('human', 'brain'), ('brain', 'thinks'), ('thinks', ','), (',', 'and'), ('and', 'how'), ('how', 'humans'), ('humans', 'learn'), ('learn', ','), (',', 'decide'), ('decide', ','), (',', 'and'), ('and', 'work'), ('work', 'while'), ('while', 'trying'), ('trying', 'to'), ('to', 'solve'), ('solve', 'a'), ('a', 'problem'), ('problem', ','), (',', 'and'), ('and', 'then'), ('then', 'using'), ('using', 'the'), ('the', 'outcomes'), ('outcomes', 'of'), ('of', 'this'), ('this', 'study'), ('study', 'as'), ('as', 'a'), ('a', 'basis'), ('basis', 'of'), ('of', 'developing'), ('developing', 'intelligent'), ('intelligent', 'software'), ('software', 'and'), ('and', 'systems'), ('systems', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quotes_bigrams)\n",
    "len(quotes_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('According', 'to', 'the'), ('to', 'the', 'father'), ('the', 'father', 'of'), ('father', 'of', 'Artificial'), ('of', 'Artificial', 'Intelligence'), ('Artificial', 'Intelligence', ','), ('Intelligence', ',', 'John'), (',', 'John', 'McCarthy'), ('John', 'McCarthy', ','), ('McCarthy', ',', 'it'), (',', 'it', 'is'), ('it', 'is', '“'), ('is', '“', 'The'), ('“', 'The', 'science'), ('The', 'science', 'and'), ('science', 'and', 'engineering'), ('and', 'engineering', 'of'), ('engineering', 'of', 'making'), ('of', 'making', 'intelligent'), ('making', 'intelligent', 'machines'), ('intelligent', 'machines', ','), ('machines', ',', 'especially'), (',', 'especially', 'intelligent'), ('especially', 'intelligent', 'computer'), ('intelligent', 'computer', 'programs'), ('computer', 'programs', '”'), ('programs', '”', '.'), ('”', '.', 'Artificial'), ('.', 'Artificial', 'Intelligence'), ('Artificial', 'Intelligence', 'is'), ('Intelligence', 'is', 'a'), ('is', 'a', 'way'), ('a', 'way', 'of'), ('way', 'of', 'making'), ('of', 'making', 'a'), ('making', 'a', 'computer'), ('a', 'computer', ','), ('computer', ',', 'a'), (',', 'a', 'computer-controlled'), ('a', 'computer-controlled', 'robot'), ('computer-controlled', 'robot', ','), ('robot', ',', 'or'), (',', 'or', 'a'), ('or', 'a', 'software'), ('a', 'software', 'think'), ('software', 'think', 'intelligently'), ('think', 'intelligently', ','), ('intelligently', ',', 'in'), (',', 'in', 'the'), ('in', 'the', 'similar'), ('the', 'similar', 'manner'), ('similar', 'manner', 'the'), ('manner', 'the', 'intelligent'), ('the', 'intelligent', 'humans'), ('intelligent', 'humans', 'think'), ('humans', 'think', '.'), ('think', '.', 'AI'), ('.', 'AI', 'is'), ('AI', 'is', 'accomplished'), ('is', 'accomplished', 'by'), ('accomplished', 'by', 'studying'), ('by', 'studying', 'how'), ('studying', 'how', 'human'), ('how', 'human', 'brain'), ('human', 'brain', 'thinks'), ('brain', 'thinks', ','), ('thinks', ',', 'and'), (',', 'and', 'how'), ('and', 'how', 'humans'), ('how', 'humans', 'learn'), ('humans', 'learn', ','), ('learn', ',', 'decide'), (',', 'decide', ','), ('decide', ',', 'and'), (',', 'and', 'work'), ('and', 'work', 'while'), ('work', 'while', 'trying'), ('while', 'trying', 'to'), ('trying', 'to', 'solve'), ('to', 'solve', 'a'), ('solve', 'a', 'problem'), ('a', 'problem', ','), ('problem', ',', 'and'), (',', 'and', 'then'), ('and', 'then', 'using'), ('then', 'using', 'the'), ('using', 'the', 'outcomes'), ('the', 'outcomes', 'of'), ('outcomes', 'of', 'this'), ('of', 'this', 'study'), ('this', 'study', 'as'), ('study', 'as', 'a'), ('as', 'a', 'basis'), ('a', 'basis', 'of'), ('basis', 'of', 'developing'), ('of', 'developing', 'intelligent'), ('developing', 'intelligent', 'software'), ('intelligent', 'software', 'and'), ('software', 'and', 'systems'), ('and', 'systems', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quotes_trigrams)\n",
    "len(quotes_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('According', 'to', 'the', 'father', 'of'), ('to', 'the', 'father', 'of', 'Artificial'), ('the', 'father', 'of', 'Artificial', 'Intelligence'), ('father', 'of', 'Artificial', 'Intelligence', ','), ('of', 'Artificial', 'Intelligence', ',', 'John'), ('Artificial', 'Intelligence', ',', 'John', 'McCarthy'), ('Intelligence', ',', 'John', 'McCarthy', ','), (',', 'John', 'McCarthy', ',', 'it'), ('John', 'McCarthy', ',', 'it', 'is'), ('McCarthy', ',', 'it', 'is', '“'), (',', 'it', 'is', '“', 'The'), ('it', 'is', '“', 'The', 'science'), ('is', '“', 'The', 'science', 'and'), ('“', 'The', 'science', 'and', 'engineering'), ('The', 'science', 'and', 'engineering', 'of'), ('science', 'and', 'engineering', 'of', 'making'), ('and', 'engineering', 'of', 'making', 'intelligent'), ('engineering', 'of', 'making', 'intelligent', 'machines'), ('of', 'making', 'intelligent', 'machines', ','), ('making', 'intelligent', 'machines', ',', 'especially'), ('intelligent', 'machines', ',', 'especially', 'intelligent'), ('machines', ',', 'especially', 'intelligent', 'computer'), (',', 'especially', 'intelligent', 'computer', 'programs'), ('especially', 'intelligent', 'computer', 'programs', '”'), ('intelligent', 'computer', 'programs', '”', '.'), ('computer', 'programs', '”', '.', 'Artificial'), ('programs', '”', '.', 'Artificial', 'Intelligence'), ('”', '.', 'Artificial', 'Intelligence', 'is'), ('.', 'Artificial', 'Intelligence', 'is', 'a'), ('Artificial', 'Intelligence', 'is', 'a', 'way'), ('Intelligence', 'is', 'a', 'way', 'of'), ('is', 'a', 'way', 'of', 'making'), ('a', 'way', 'of', 'making', 'a'), ('way', 'of', 'making', 'a', 'computer'), ('of', 'making', 'a', 'computer', ','), ('making', 'a', 'computer', ',', 'a'), ('a', 'computer', ',', 'a', 'computer-controlled'), ('computer', ',', 'a', 'computer-controlled', 'robot'), (',', 'a', 'computer-controlled', 'robot', ','), ('a', 'computer-controlled', 'robot', ',', 'or'), ('computer-controlled', 'robot', ',', 'or', 'a'), ('robot', ',', 'or', 'a', 'software'), (',', 'or', 'a', 'software', 'think'), ('or', 'a', 'software', 'think', 'intelligently'), ('a', 'software', 'think', 'intelligently', ','), ('software', 'think', 'intelligently', ',', 'in'), ('think', 'intelligently', ',', 'in', 'the'), ('intelligently', ',', 'in', 'the', 'similar'), (',', 'in', 'the', 'similar', 'manner'), ('in', 'the', 'similar', 'manner', 'the'), ('the', 'similar', 'manner', 'the', 'intelligent'), ('similar', 'manner', 'the', 'intelligent', 'humans'), ('manner', 'the', 'intelligent', 'humans', 'think'), ('the', 'intelligent', 'humans', 'think', '.'), ('intelligent', 'humans', 'think', '.', 'AI'), ('humans', 'think', '.', 'AI', 'is'), ('think', '.', 'AI', 'is', 'accomplished'), ('.', 'AI', 'is', 'accomplished', 'by'), ('AI', 'is', 'accomplished', 'by', 'studying'), ('is', 'accomplished', 'by', 'studying', 'how'), ('accomplished', 'by', 'studying', 'how', 'human'), ('by', 'studying', 'how', 'human', 'brain'), ('studying', 'how', 'human', 'brain', 'thinks'), ('how', 'human', 'brain', 'thinks', ','), ('human', 'brain', 'thinks', ',', 'and'), ('brain', 'thinks', ',', 'and', 'how'), ('thinks', ',', 'and', 'how', 'humans'), (',', 'and', 'how', 'humans', 'learn'), ('and', 'how', 'humans', 'learn', ','), ('how', 'humans', 'learn', ',', 'decide'), ('humans', 'learn', ',', 'decide', ','), ('learn', ',', 'decide', ',', 'and'), (',', 'decide', ',', 'and', 'work'), ('decide', ',', 'and', 'work', 'while'), (',', 'and', 'work', 'while', 'trying'), ('and', 'work', 'while', 'trying', 'to'), ('work', 'while', 'trying', 'to', 'solve'), ('while', 'trying', 'to', 'solve', 'a'), ('trying', 'to', 'solve', 'a', 'problem'), ('to', 'solve', 'a', 'problem', ','), ('solve', 'a', 'problem', ',', 'and'), ('a', 'problem', ',', 'and', 'then'), ('problem', ',', 'and', 'then', 'using'), (',', 'and', 'then', 'using', 'the'), ('and', 'then', 'using', 'the', 'outcomes'), ('then', 'using', 'the', 'outcomes', 'of'), ('using', 'the', 'outcomes', 'of', 'this'), ('the', 'outcomes', 'of', 'this', 'study'), ('outcomes', 'of', 'this', 'study', 'as'), ('of', 'this', 'study', 'as', 'a'), ('this', 'study', 'as', 'a', 'basis'), ('study', 'as', 'a', 'basis', 'of'), ('as', 'a', 'basis', 'of', 'developing'), ('a', 'basis', 'of', 'developing', 'intelligent'), ('basis', 'of', 'developing', 'intelligent', 'software'), ('of', 'developing', 'intelligent', 'software', 'and'), ('developing', 'intelligent', 'software', 'and', 'systems'), ('intelligent', 'software', 'and', 'systems', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quotes_ngrams)\n",
    "len(quotes_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.', 'Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.', 'AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "AI_tokens = sent_tokenize(AI)\n",
    "print(AI_tokens)\n",
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown_bigrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0a88ee9ac136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfdist2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrown_bigrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfdist2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'brown_bigrams' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist2 = FreqDist()\n",
    "for bigrams in brown_bigrams:\n",
    "    fdist2[bigrams.lower()]+=1\n",
    "print(fdist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CategorizedTaggedCorpusReader.words of <CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>>\n"
     ]
    }
   ],
   "source": [
    "hamlet2=nltk.corpus.brown.words\n",
    "print(hamlet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.stem' has no attribute 'download'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-5c4285fadfea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk.stem' has no attribute 'download'"
     ]
    }
   ],
   "source": [
    "nltk.stem.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giving: give\n",
      "given: given\n",
      "gave: gave\n"
     ]
    }
   ],
   "source": [
    "pst = PorterStemmer()\n",
    "pst.stem(\"having\")\n",
    "words_to_stem=[\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+ \": \" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giving: give\n",
      "given: given\n",
      "gave: gave\n"
     ]
    }
   ],
   "source": [
    "pst2 = SnowballStemmer(\"english\")\n",
    "pst2.stem(\"having\")\n",
    "words_to_stem=[\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+ \": \" +pst2.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: buy\n",
      "buying: buy\n",
      "buyer: buyer\n",
      "bought: bought\n"
     ]
    }
   ],
   "source": [
    "pst3 = PorterStemmer()\n",
    "pst3.stem(\"buying\")\n",
    "words_to_stem=[\"buy\",\"buying\",\"buyer\",\"bought\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+ \": \" +pst3.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: buy\n",
      "buying: buy\n",
      "buyer: buyer\n",
      "bought: bought\n"
     ]
    }
   ],
   "source": [
    "pst4 = SnowballStemmer(\"english\")\n",
    "pst4.stem(\"buying\")\n",
    "words_to_stem=[\"buy\",\"buying\",\"buyer\",\"bought\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+ \": \" +pst4.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giving: giving\n",
      "given: given\n",
      "gave: gave\n"
     ]
    }
   ],
   "source": [
    "words_to_lemm = [\"give\",\"giving\",\"given\",\"gave\"]\n",
    "word_lem = WordNetLemmatizer()\n",
    "for words in words_to_lemm:\n",
    "    print(words+ \": \" +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: buy\n",
      "buying: buying\n",
      "buyer: buyer\n",
      "bought: bought\n"
     ]
    }
   ],
   "source": [
    "words_to_lemm = [\"buy\",\"buying\",\"buyer\",\"bought\"]\n",
    "word_lem = WordNetLemmatizer()\n",
    "for words in words_to_lemm:\n",
    "    print(words+ \": \" +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "aw = stopwords.words('english')\n",
    "print (aw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "aw = stopwords.words('english')\n",
    "print(aw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.',\n",
       " 'Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.',\n",
       " 'AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "aw = stopwords.words('english')\n",
    "post_punctuation=[]\n",
    "for words in AI_tokens:\n",
    "    if words not in aw:\n",
    "        post_punctuation.append(words)\n",
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”',\n",
       " 'Artificial Intelligence is a way of making a computer, a computercontrolled robot, or a software think intelligently, in the similar manner the intelligent humans think',\n",
       " 'AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "punctuation=re.compile(r'[-.?!.:;()|0-9]')\n",
    "post_punctuation=[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation.sub(\"\", words)\n",
    "    if len(word) > 0:\n",
    "        post_punctuation.append(word)\n",
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('[-.?!.:;()|0-9]')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "punctuation2=re.compile(r'[-.?!.:;()|0-9]')\n",
    "print (punctuation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”\n",
      "Artificial Intelligence is a way of making a computer, a computercontrolled robot, or a software think intelligently, in the similar manner the intelligent humans think\n",
      "AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”',\n",
       " 'Artificial Intelligence is a way of making a computer, a computercontrolled robot, or a software think intelligently, in the similar manner the intelligent humans think',\n",
       " 'AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems',\n",
       " 'According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”',\n",
       " 'Artificial Intelligence is a way of making a computer, a computercontrolled robot, or a software think intelligently, in the similar manner the intelligent humans think',\n",
       " 'AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "punctuation2=re.compile(r'[-.?!.:;()|0-9]')\n",
    "post_punctuation2=[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation2.sub(\"\", words)\n",
    "    print (word)\n",
    "    if len(word) > 0:\n",
    "        post_punctuation.append(word)\n",
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”\n",
      "Artificial Intelligence is a way of making a computer, a computercontrolled robot, or a software think intelligently, in the similar manner the intelligent humans think\n",
      "AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems\n"
     ]
    }
   ],
   "source": [
    "punctuation3=re.compile(r'[-.?!.:;()|0-9]')\n",
    "post_punctuation3=[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation3.sub(\"\", words)\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "punctuation=re.compile(r'[-.?!.:;()|0-9]')\n",
    "post_punctuation=[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation.sub(\"\", words)\n",
    "    if len(word) > 0:\n",
    "        post_punctuation.append(word)\n",
    "post_punctuation\n",
    "len(post_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accrding   ar  Ariicial Inllignc, Jn McCary, i is “T scinc and nginring  making inllign macins, spcially inllign cmpur prgrams”.',\n",
       " 'Ariicial Inllignc is a way  making a cmpur, a cmpur-cnrlld rb, r a swar ink inllignly, in  similar mannr  inllign umans ink.',\n",
       " 'AI is accmplisd by sudying w uman brain inks, and w umans larn, dcid, and wrk wil rying  slv a prblm, and n using  ucms  is sudy as a basis  dvlping inllign swar and sysms.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "punctuation=re.compile(r'[the|of]')\n",
    "post_punctuation=[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation.sub(\"\", words)\n",
    "    if len(word) > 0:\n",
    "        post_punctuation.append(word)\n",
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "punctuation=re.compile(r'[the]')\n",
    "post_punctuation=[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation.sub(\"\", words)\n",
    "    if len(word) > 0:\n",
    "        post_punctuation.append(word)\n",
    "post_punctuation\n",
    "len(post_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Timothy', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('drawing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"Timothy is a natural when it comes to drawing\"\n",
    "sent_tokens = word_tokenize(sent)\n",
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('eating', 'VBG')]\n",
      "[('a', 'DT')]\n",
      "[('delicious', 'JJ')]\n",
      "[('cake', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent2 = \"John is eating a delicious cake\"\n",
    "sent_tokens2 = word_tokenize(sent2)\n",
    "for token in sent_tokens2:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "NE_sent=\"The US President stays in the WHITE HOUSE\"\n",
    "NE_tokens=word_tokenize(NE_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
     ]
    }
   ],
   "source": [
    "NE_tags=nltk.pos_tag(NE_tokens)\n",
    "NE_NER=ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Google/NNP)\n",
      "  ’/NNP\n",
      "  s/VBD\n",
      "  (ORGANIZATION CEO/NNP Sundar/NNP Pichai/NNP)\n",
      "  introduce/VB\n",
      "  the/DT\n",
      "  new/JJ\n",
      "  Pixel/NNP\n",
      "  3/CD\n",
      "  at/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  Central/NNP\n",
      "  Mall/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "NE_sent2=\"Google’s CEO Sundar Pichai introduce the new Pixel 3 at New York Central Mall\"\n",
    "NE_tokens2=word_tokenize(NE_sent2)\n",
    "NE_tags2=nltk.pos_tag(NE_tokens2)\n",
    "NE_NER2=ne_chunk(NE_tags2)\n",
    "print(NE_NER2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=\"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens=nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('watch', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('Lion', 'NNP'),\n",
       " ('King', 'NNP'),\n",
       " ('tonight', 'NN')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new2=\"I watch the Lion King tonight\"\n",
    "new_tokens2=nltk.pos_tag(word_tokenize(new2))\n",
    "new_tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np2=r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar_np2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP watch/VBP the/DT Lion/NNP King/NNP (NP tonight/NN))\n"
     ]
    }
   ],
   "source": [
    "chunk_result2=chunk_parser.parse(new_tokens2)\n",
    "print(chunk_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP watch/VBP (NP the/DT) Lion/NNP King/NNP (NP tonight/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar_np3=r\"NP: {<DT>?<JJ>*<NN>*}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar_np3)\n",
    "chunk_result3=chunk_parser.parse(new_tokens2)\n",
    "print(chunk_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP watch/VBP (NP the/DT Lion/NNP King/NNP) (NP tonight/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar_np4=r\"NP: {<DT>?<JJ>*<NNP>*|<NN>*}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar_np4)\n",
    "chunk_result4=chunk_parser.parse(new_tokens2)\n",
    "print(chunk_result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Saya', 'NNP'),\n",
       " ('makan', 'PRP'),\n",
       " ('nasi', 'RB'),\n",
       " ('padang', 'VBD'),\n",
       " ('di', 'JJ'),\n",
       " ('Bekasi', 'NNP')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bahasa=\"Saya makan nasi padang di Bekasi\"\n",
    "bahasa_tokens=nltk.pos_tag(word_tokenize(bahasa))\n",
    "bahasa_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S Saya/PRP makan/VBP (NP nasi/NN padang/NNP))\n"
     ]
    }
   ],
   "source": [
    "indo_example = [('Saya', 'PRP'), ('makan', 'VBP'), ('nasi', 'NN'), ('padang', 'NNP')]\n",
    "bahasa_np=r\"NP: {<NN><NNP>*}\"\n",
    "chunk_parser=nltk.RegexpParser(bahasa_np)\n",
    "chunk_result5=chunk_parser.parse(indo_example)\n",
    "print(chunk_result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bahasa_np=r\"NP: {<DT>?<JJ>*<NN>*}\"\n",
    "chunk_parser=nltk.RegexpParser(bahasa_np)\n",
    "chunk_result5=chunk_parser.parse(bahasa_tokens)\n",
    "print(chunk_result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP saya/NN)\n",
      "  (NP makan/NN)\n",
      "  (NP nasi/NN)\n",
      "  (NP padang/NN)\n",
      "  (NP di/NN)\n",
      "  Bekasi/NNP)\n"
     ]
    }
   ],
   "source": [
    "bahasa_np=r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser=nltk.RegexpParser(bahasa_np)\n",
    "chunk_result5=chunk_parser.parse(bahasa_tokens)\n",
    "print(chunk_result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-62-ca14c17f3ee4>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-62-ca14c17f3ee4>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    'When my cat sits down, she looks like a Furby toy!’,\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'All my cats in a row',\n",
    "    'When my cat sits down, she looks like a Furby toy!’,\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-ab2cb19cb604>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-85-ab2cb19cb604>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    from gensim.summarization import keywordsfrom nltk.tokenize\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywordsfrom nltk.tokenize\n",
    "import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
